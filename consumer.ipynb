{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-TH9Q9P7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>kafka-example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x253e5b37cd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "scala_version = '2.12'  # your scala version\n",
    "spark_version = '3.5.0' # your spark version\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:2.8.0' #your kafka version\n",
    "]\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"kafka-example\").config(\"spark.jars.packages\", \",\".join(packages)).getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0',\n",
       " 'org.apache.kafka:kafka-clients:2.8.0']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'text3'\n",
    "kafka_server = 'localhost:9092'\n",
    "\n",
    "kafkaDf = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\", topic_name).option(\"startingOffsets\", \"earliest\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "| key|               value|topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     0|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     1|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     2|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     3|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     4|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     5|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     6|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     7|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     8|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|     9|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    10|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    11|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    12|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    13|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    14|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    15|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    16|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    17|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    18|2024-01-20 23:11:...|            0|\n",
      "|NULL|[31 39 39 39 2D 3...|text3|        0|    19|2024-01-20 23:11:...|            0|\n",
      "+----+--------------------+-----+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafkaDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1570"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kafkaDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "import numpy as np\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler\n",
    "from time import sleep\n",
    "from IPython.display import display, clear_output\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import from_json, split, current_date, year, col, when\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = spark.read.csv(\"./data/train.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+------------------+---------+\n",
      "|      Date|              Open|              High|               Low|             Close|         Adj Close|   Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+---------+\n",
      "|2000-12-21|1.3098959922790527|1.4895830154418943|1.1458330154418943|1.1614580154418943| 1.065457820892334| 84525600|\n",
      "|2008-03-25|  5.03000020980835| 5.175000190734863| 4.982500076293945| 5.079999923706055| 4.660114288330078| 73538800|\n",
      "|2018-03-08| 60.73749923706055| 60.73749923706055| 59.84749984741211| 60.29499816894531|      59.697265625| 41191200|\n",
      "|2019-02-21| 39.76499938964844| 40.01250076293945| 38.79499816894531|38.942501068115234| 38.64001083374024| 44854800|\n",
      "|2006-07-07|3.3550000190734863|3.3616669178009038| 3.191667079925537| 3.228332996368408|2.9614956378936768| 54123000|\n",
      "|2004-04-12| 2.245832920074463|  2.25583291053772| 2.173332929611206| 2.186666965484619|2.0059287548065186| 44378400|\n",
      "|2018-03-09|60.775001525878906|61.462501525878906| 60.61249923706055| 61.33250045776367| 60.72447204589844| 50552400|\n",
      "|2019-03-26|44.872501373291016|           45.4375|43.650001525878906| 44.21749877929688| 43.91925048828125| 70350800|\n",
      "|2006-01-23| 3.679167032241821| 3.702500104904175| 3.629167079925537| 3.671667098999024|3.3681859970092773| 33488400|\n",
      "|2007-11-16|               8.0| 8.135000228881836| 7.934999942779541| 8.112500190734863| 7.441962242126465| 42359200|\n",
      "|2004-08-12| 0.862500011920929|0.8650000095367432| 0.838333010673523|0.8399999737739563|0.7705699801445007| 65438400|\n",
      "|2014-03-17|  4.46750020980835|               4.5| 4.449999809265137| 4.454999923706055| 4.221982479095459| 29965600|\n",
      "|2021-03-15| 128.6475067138672| 132.0675048828125|127.67250061035156|131.91250610351562| 131.6613006591797| 22198800|\n",
      "|2016-10-17|  16.4950008392334|16.649999618530273|  16.3799991607666| 16.40250015258789|16.142650604248047| 19532800|\n",
      "|1999-05-05|0.3645829856395721|0.3697920143604278|           0.34375|0.3697920143604278|0.3392269015312195| 14457600|\n",
      "|2009-09-30| 3.714999914169312| 3.802500009536743| 3.632499933242798| 3.757499933242798|3.4469244480133057| 87595600|\n",
      "|2007-12-04|  8.03499984741211|            8.0625| 7.837500095367432| 7.992499828338623| 7.331881999969482| 37171200|\n",
      "|2001-08-13| 3.587083101272583| 3.714582920074463| 3.533750057220459| 3.674582958221436| 3.370861768722534|151478400|\n",
      "|2006-12-20|6.4166669845581055| 6.493332862854004|  6.37166690826416| 6.376667022705078| 5.849604606628418| 30659400|\n",
      "|2013-01-03| 3.180000066757202|3.2174999713897705|3.1449999809265137| 3.182499885559082| 2.938286781311035| 29888800|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = trainDF \\\n",
    "    .withColumn(\"Date\", col(\"Date\").cast(\"date\")) \\\n",
    "    .withColumn(\"Open\", col(\"Open\").cast(DoubleType())) \\\n",
    "    .withColumn(\"High\", col(\"High\").cast(DoubleType())) \\\n",
    "    .withColumn(\"Low\", col(\"Low\").cast(DoubleType())) \\\n",
    "    .withColumn(\"Close\", col(\"Close\").cast(DoubleType())) \\\n",
    "    .withColumn(\"Adj Close\", col(\"Adj Close\").cast(DoubleType())) \\\n",
    "    .withColumn(\"Volume\", col(\"Volume\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo một đối tượng VectorAssembler với các cột đầu vào Open, High, Low, Volume và cột đầu ra Features\n",
    "featureAssembler = VectorAssembler(inputCols=[\"Open\", \"High\", \"Low\", \"Volume\"], outputCol=\"Features\")\n",
    "\n",
    "# Áp dụng VectorAssembler để tạo cột \"Features\" trong dữ liệu đầu vào\n",
    "output = featureAssembler.transform(trainDF)\n",
    "\n",
    "# Tạo một đối tượng MinMaxScaler với cột đầu vào là \"Features\" và cột đầu ra là \"ScaledFeatures\", đặt phạm vi chuẩn hóa là [-1;1]\n",
    "scaler = MinMaxScaler(inputCol=\"Features\", outputCol=\"ScaledFeatures\", min=-1, max=1)\n",
    "\n",
    "# Fit dữ liệu vào mô hình scaler để tính toán các tham số chuẩn hóa\n",
    "scalerModel = scaler.fit(output)\n",
    "\n",
    "# Áp dụng mô hình scaler trên vào dữ liệu đầu vào \"output\"\n",
    "scaledOutput = scalerModel.transform(output)\n",
    "\n",
    "#Tạo dữ liệu để train model gồm 3 cột \"Date\", \"ScaledFeatures\" và \"Close\"\n",
    "finalData = scaledOutput.select(\"Date\", \"ScaledFeatures\", \"Close\")\n",
    "\n",
    "LR = LinearRegression(featuresCol='ScaledFeatures', labelCol='Close')\n",
    "LRModel = LR.fit(finalData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing live view refreshed every 5 seconds\n",
      "Seconds passed: 10\n",
      "+----------+-------------------+\n",
      "|      Date|              Close|\n",
      "+----------+-------------------+\n",
      "|1999-01-22| 0.4379495649727403|\n",
      "|1999-01-25|0.43024622753577546|\n",
      "|1999-02-02|0.37691723732868354|\n",
      "|1999-02-04| 0.4052494454867315|\n",
      "|1999-02-17|0.41638425459424866|\n",
      "|1999-02-18|  0.418623667698256|\n",
      "|1999-02-25| 0.4934644793627285|\n",
      "|1999-03-02| 0.4545102584355618|\n",
      "|1999-03-12|  0.422125571173126|\n",
      "|1999-03-23| 0.3985540484536614|\n",
      "|1999-04-01| 0.4272994656195124|\n",
      "|1999-04-05| 0.4244203235649593|\n",
      "|1999-04-21| 0.3951216926840857|\n",
      "|1999-04-22| 0.3795331779069784|\n",
      "|1999-04-27| 0.3730052702600233|\n",
      "|1999-04-28| 0.3684417164308229|\n",
      "|1999-05-07| 0.3673155497137941|\n",
      "|1999-05-10|0.37344423599279253|\n",
      "|1999-06-02|0.34831890875523186|\n",
      "|1999-06-09|0.38337079394446505|\n",
      "+----------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "break\n",
      "Live view ended...\n"
     ]
    }
   ],
   "source": [
    "for x in range(0, 2000):\n",
    "    try:\n",
    "        print(\"Showing live view refreshed every 5 seconds\")\n",
    "        print(f\"Seconds passed: {x*5}\")\n",
    "        df = kafkaDf.selectExpr(\"CAST(value AS STRING)\").select(split(\"value\", \",\").alias(\"csv_values\")) \\\n",
    "                        .selectExpr(\"csv_values[0] as Date\", \"csv_values[1] as Open\", \\\n",
    "                                    \"csv_values[2] as High\", \"csv_values[3] as Low\", \\\n",
    "                                    \"csv_values[4] as Adj_close\", \"csv_values[5] as Volume\")\n",
    "\n",
    "        df1 = df.alias(\"copied\")\n",
    "        df1 = df1 \\\n",
    "            .withColumn(\"Date\", col(\"Date\").cast(\"date\")) \\\n",
    "            .withColumn(\"Open\", col(\"Open\").cast(DoubleType())) \\\n",
    "            .withColumn(\"High\", col(\"High\").cast(DoubleType())) \\\n",
    "            .withColumn(\"Low\", col(\"Low\").cast(DoubleType())) \\\n",
    "            .withColumn(\"Adj_close\", col(\"Adj_close\").cast(DoubleType())) \\\n",
    "            .withColumn(\"Volume\", col(\"Volume\").cast(IntegerType()))\n",
    "\n",
    "        # Áp dụng VectorAssembler để tạo cột \"Features\" trong dữ liệu đầu vào\n",
    "        output_test = featureAssembler.transform(df1)\n",
    "\n",
    "        # Áp dụng mô hình scaler trên vào dữ liệu đầu vào \"output\"\n",
    "        scaledOutput_test = scalerModel.transform(output_test)\n",
    "\n",
    "        #Tạo dữ liệu để train model gồm 3 cột \"Date\", \"ScaledFeatures\" và \"Close\"\n",
    "        finalData_test = scaledOutput_test.select(\"Date\", \"ScaledFeatures\")\n",
    "\n",
    "        LRPredictions = LRModel.transform(finalData_test)\n",
    "        result = LRPredictions.select(\"Date\",\"prediction\")\n",
    "        result = result.withColumnRenamed(\"prediction\", \"Close\")\n",
    "\n",
    "        result.show()\n",
    "        sleep(5)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"break\")\n",
    "        break\n",
    "print(\"Live view ended...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
